{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 152: Intro to Computer Vision - Spring 2019 Assignment 4\n",
    "## Instructor: David Kriegman\n",
    "### Assignment published on Wednesday, May 29, 2019\n",
    "### Due on Friday, June 7, 2019 at 11:59pm\n",
    "\n",
    "## Instructions\n",
    "* This assignment must be completed individually. Review the academic integrity and collaboration policies on the course website.\n",
    "* All solutions should be written in this notebook. Show your work for written questions.\n",
    "* If you want to modify the skeleton code, you may do so. It has been merely been provided as a framework for your solution.\n",
    "* You may use Python packages for basic linear algebra (e.g. NumPy or SciPy for basic operations), but you may not use packages that directly solve the problem. If you are unsure about using a specific package or function, ask the instructor and/or teaching assistants for clarification.\n",
    "* You must submit this notebook exported as a PDF. You must also submit this notebook as an `.ipynb` file. Submit both files (`.pdf` and `.ipynb`) on Gradescope. **You must mark the PDF pages associated with each question in Gradescope. If you fail to do so, we may dock points.**\n",
    "* It is highly recommended that you begin working on this assignment early.\n",
    "* **Late policy:** a penalty of 10% per day after the due date.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Machine Learning [10 pts]\n",
    "\n",
    "In this problem, you will implement K-Nearest Neighbors (KNN) algorithm for computer vision problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data preparation [1 pts]\n",
    "\n",
    "Download the MNIST data from http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "Download the 4 zipped files, extract them into one folder, and change the variable 'path' in the code below. (Code taken from https://gist.github.com/akesling/5358964 )\n",
    "\n",
    "Plot one random example image corresponding to each label from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "# Change path as required\n",
    "path = \"./mnist_data/\"\n",
    "\n",
    "def read(dataset=\"training\", datatype='images'):\n",
    "    \"\"\"\n",
    "    Python function for importing the MNIST data set.  It returns an iterator\n",
    "    of 2-tuples with the first element being the label and the second element\n",
    "    being a numpy.uint8 2D array of pixel data for the given image.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels.idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)\n",
    "    \n",
    "    if(datatype=='images'):\n",
    "        get_data = lambda idx: img[idx]\n",
    "    elif(datatype=='labels'):\n",
    "        get_data = lambda idx: lbl[idx]\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_data(i)\n",
    "        \n",
    "trainData=np.array(list(read('training','images')))\n",
    "trainLabels=np.array(list(read('training','labels')))\n",
    "testData=np.array(list(read('testing','images')))\n",
    "testLabels=np.array(list(read('testing','labels')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a generator for batches of data\n",
    "# yields data (batchsize, 3, 32, 32) and labels (batchsize)\n",
    "# if shuffle, it will load batches in a random order\n",
    "import matplotlib.pyplot as plt\n",
    "def DataBatch(data, label, batchsize, shuffle=True):\n",
    "    n = data.shape[0]\n",
    "    if shuffle:\n",
    "        index = np.random.permutation(n)\n",
    "    else:\n",
    "        index = np.arange(n)\n",
    "    for i in range(int(np.ceil(n/batchsize))):\n",
    "        inds = index[i*batchsize : min(n,(i+1)*batchsize)]\n",
    "        yield data[inds], label[inds]\n",
    "\n",
    "# tests the accuracy of a classifier\n",
    "def test(testData, testLabels, classifier):\n",
    "    batchsize=50\n",
    "    correct=0.\n",
    "    for data,label in DataBatch(testData,testLabels,batchsize,shuffle=False):\n",
    "        prediction = classifier(data)\n",
    "        correct += np.sum(prediction==label)\n",
    "    return correct/testData.shape[0]*100\n",
    "\n",
    "# a sample classifier\n",
    "# given an input it outputs a random class\n",
    "class RandomClassifier():\n",
    "    def __init__(self, classes=10):\n",
    "        self.classes=classes\n",
    "    def __call__(self, x):\n",
    "        return np.random.randint(self.classes, size=x.shape[0])\n",
    "\n",
    "randomClassifier = RandomClassifier()\n",
    "print('Random classifier accuracy: %f \\n' % \n",
    "      test(testData, testLabels, randomClassifier))\n",
    "\n",
    "print('Plot random training images for each class:')\n",
    "count = 0\n",
    "check = np.zeros(10)\n",
    "imgs = np.zeros((10, 28, 28))\n",
    "for img, lb in DataBatch(trainData, trainLabels, 1, shuffle=True):\n",
    "    img = np.squeeze(img)\n",
    "    if check[lb] == 1:\n",
    "        continue\n",
    "    else:\n",
    "        check[lb] += 1\n",
    "        count += 1\n",
    "        imgs[lb,:,:] = img\n",
    "    if count == 10:\n",
    "        break\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5)\n",
    "i = 0\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        col.imshow(imgs[i,:,:])\n",
    "        i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Confusion Matrix [3 pts]\n",
    "Here you will implement a function that computes the confusion matrix for a classifier.\n",
    "The matrix (M) should be nxn where n is the number of classes.\n",
    "Entry M[i,j] should contain the fraction of images of class i that were classified as class j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the tqdm module to visualize run time is suggested\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# It would be a good idea to return the accuracy, along with the confusion \n",
    "# matrix, since both can be calculated in one iteration over test data, to \n",
    "# save time\n",
    "def Confusion(testData, testLabels, classifier):\n",
    "    '''\n",
    "    Your code here\n",
    "    '''\n",
    "    M = np.zeros((10,10))\n",
    "\n",
    "    return M, accuracy\n",
    "\n",
    "def VisualizeConfusion(M):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.imshow(M)\n",
    "    plt.show()\n",
    "    print(np.round(M,2))\n",
    "    \n",
    "M, _ = Confusion(testData, testLabels, randomClassifier)\n",
    "VisualizeConfusion(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: K-Nearest Neighbors (KNN) [6 pts]\n",
    "\n",
    "* Here you will implement a simple knn classifier. The distance metric is Euclidean in pixel space. k refers to the number of neighbors involved in voting on the class, and should be 3. You are allowed to use sklearn.neighbors.KNeighborsClassifier.\n",
    "* Display the confusion matrix and accuracy for your KNN classifier trained on the entire training dataset. (should be ~97%)\n",
    "* After evaluating the classifier on the test set, based on the confusion matrix, mention the number that the number '4' is most often predicted to be, other than '4'. Write your comment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class KNNClassifer():\n",
    "    def __init__(self, k=3):\n",
    "        # k is the number of neighbors involved in voting\n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "\n",
    "    def train(self, trainData, trainLabels):\n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # this method should take a batch of images\n",
    "        # and return a batch of predictions\n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "\n",
    "# test your classifier with only the first 100 training examples (use this\n",
    "# while debugging)\n",
    "# note you should get ~ 65 % accuracy\n",
    "knnClassiferX = KNNClassifer()\n",
    "knnClassiferX.train(trainData[:100], trainLabels[:100])\n",
    "print ('KNN classifier accuracy: %f'%test(testData, testLabels, knnClassiferX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your classifier trained with all the training examples (This may take a while)\n",
    "knnClassifer = KNNClassifer()\n",
    "knnClassifer.train(trainData[:-1], trainLabels[:-1])\n",
    "\n",
    "# display confusion matrix and testing accuracy for your KNN classifier trained with all the training examples\n",
    "'''\n",
    "your code here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment:\n",
    "Your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Deep Learning [18 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Initial setup [0 pts]\n",
    "\n",
    "Follow the directions on https://pytorch.org/get-started/locally/ to install PyTorch on your computer.\n",
    "\n",
    "Note: You will not need GPU support for this assignment so don't worry if you don't have one. In any case, installing with GPU support is often more difficult to configure, so it is suggested that you install the CPU-only version regardless.\n",
    "\n",
    "To ensure that PyTorch was installed correctly, we will now verify the installation by running some sample PyTorch code. Here we construct a randomly initialized tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2:  Training with PyTorch [3 pts]\n",
    "Below is some helper code to train your deep networks. \n",
    "Complete the train function for PTClassifier below. You should write down the training operations in this function. This function will be used in the following questions with different networks.\n",
    "You can look at https://pytorch.org/tutorials/beginner/pytorch_with_examples.html for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base class for your PyTorch networks. It implements the training loop\n",
    "# (__init__), (train) and prediction(__call__)  for you.\n",
    "# You will need to complete the (train) function to define the training operations\n",
    "# structures in the following problems.\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "class PTClassifier():\n",
    "    def __init__(self, net):\n",
    "        self.net = net()\n",
    "    \n",
    "    def train(self, trainData, trainLabels, testData, testLabels, epochs=1, batchsize=50):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        learning_rate=3e-4\n",
    "        optimizer = optim.Adam(self.net.parameters(),lr=learning_rate)\n",
    "        for epoch in range(epochs):\n",
    "            for i, (data,label) in enumerate(DataBatch(trainData, trainLabels, batchsize, shuffle=True)):\n",
    "                inputs = Variable(torch.FloatTensor(data))\n",
    "                targets = Variable(torch.LongTensor(label))\n",
    "\n",
    "                # YOUR CODE HERE\n",
    "                # Train the model using the optimizer and the batch data\n",
    "\n",
    "                \n",
    "            print ('Epoch:%d Accuracy: %f'%(epoch+1, test(testData, testLabels, self)))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        inputs = Variable(torch.FloatTensor(x))\n",
    "        prediction = self.net(inputs)\n",
    "        return np.argmax(prediction.data.cpu().numpy(), 1)\n",
    "    \n",
    "    def get_first_layer_weights(self):\n",
    "        return self.net.weight1.data.cpu().numpy()\n",
    "\n",
    "# helper function to get weight variable\n",
    "def weight_variable(shape):\n",
    "    initial = torch.Tensor(truncnorm.rvs(-1/0.01, 1/0.01, scale=0.01, size=shape))\n",
    "    return Parameter(initial, requires_grad=True)\n",
    "\n",
    "# helper function to get bias variable\n",
    "def bias_variable(shape):\n",
    "    initial = torch.Tensor(np.ones(shape)*0.1)\n",
    "    return Parameter(initial, requires_grad=True)\n",
    "    \n",
    "# Define Single Layer Perceptron network\n",
    "class SLP(nn.Module):\n",
    "    def __init__(self, in_features=28*28, classes=10):\n",
    "        super(SLP, self).__init__()\n",
    "        # model variables\n",
    "        self.weight1 = weight_variable((classes, in_features))\n",
    "        self.bias1 = bias_variable((classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # linear operation\n",
    "        y_pred = torch.addmm(self.bias1, x.view(list(x.size())[0], -1), self.weight1.t())\n",
    "        return y_pred\n",
    "        \n",
    "\n",
    "# test the example linear classifier (note you should get around 92% accuracy\n",
    "# for 10 epochs and batchsize 50)\n",
    "trainData=np.array(list(read('training','images')))\n",
    "trainData=np.float32(np.expand_dims(trainData,-1))/255\n",
    "trainData=trainData.transpose((0,3,1,2))\n",
    "trainLabels=np.int32(np.array(list(read('training','labels'))))\n",
    "\n",
    "testData=np.array(list(read('testing','images')))\n",
    "testData=np.float32(np.expand_dims(testData,-1))/255\n",
    "testData=testData.transpose((0,3,1,2))\n",
    "testLabels=np.int32(np.array(list(read('testing','labels'))))\n",
    "\n",
    "linearClassifier = PTClassifier(SLP)\n",
    "linearClassifier.train(trainData, trainLabels, testData, testLabels, epochs=10)\n",
    "print ('Linear classifier accuracy: %f'%test(testData, testLabels, linearClassifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Single Layer Perceptron [3 pts]\n",
    "The simple linear classifier implemented in the cell already performs quite well. Plot the filter weights corresponding to each output class (weights, not biases) as images. (Normalize weights to lie between 0 and 1 and use color maps like 'inferno' or 'plasma' for good results). Comment on what the weights look like and why that may be so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "Your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Multi Layer Perceptron (MLP) [7 pts]\n",
    "Here you will implement an MLP. The MLP shoud consist of 2 layers (matrix multiplication and bias offset) that map to the following feature dimensions:\n",
    "\n",
    "* 28x28 -> hidden (100)\n",
    "* hidden -> classes\n",
    "\n",
    "* The hidden layer should be followed with a ReLU nonlinearity. The final layer should not have a nonlinearity applied as we desire the raw logits output.\n",
    "* The final output of the computation graph should be stored in self.y as that will be used in the training.\n",
    "\n",
    "Display the confusion matrix and accuracy after training. Note: You should get around 97% accuracy for 10 epochs and batch size 50.\n",
    "\n",
    "Plot the filter weights corresponding to the mapping from the inputs to the first 10 hidden layer outputs (out of 100). Do the weights look similar to the weights plotted in the previous problem? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Multi Layer Perceptron network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features=28*28, hidden=100, classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "\n",
    "mlpClassifer = PTClassifier(MLP)\n",
    "mlpClassifer.train(trainData, trainLabels, testData, testLabels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Accuracy\n",
    "'''\n",
    "your code here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot filter weights\n",
    "'''\n",
    "your code here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "Your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Convolutional Neural Network (CNN) [5 pts]\n",
    "Here you will implement a CNN with the following architecture:\n",
    "\n",
    "* n=5\n",
    "* ReLU( Conv(kernel_size=4x4, stride=2, output_features=n) )\n",
    "* ReLU( Conv(kernel_size=4x4, stride=2, output_features=n*2) )\n",
    "* ReLU( Conv(kernel_size=4x4, stride=2, output_features=n*4) )\n",
    "* Linear(output_features=classes)\n",
    "\n",
    "Display the confusion matrix and accuracy after training. You should get around 98% accuracy for 10 epochs and batch size 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, stride):\n",
    "    # x: input\n",
    "    # W: weights (out, in, kH, kW)\n",
    "    return F.conv2d(x, W, stride=stride, padding=1)\n",
    "\n",
    "# Define Convolutional Neural Network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, classes=10, n=5):\n",
    "        super(CNN, self).__init__()\n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "\n",
    "cnnClassifer = PTClassifier(CNN)\n",
    "cnnClassifer.train(trainData, trainLabels, testData, testLabels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Accuracy\n",
    "'''\n",
    "your code here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that the MLP/ConvNet approaches lead to an accuracy a little higher than the K-NN approach. \n",
    "* In general, neural net approaches lead to a significant increase in accuracy, but in this case the problem is not too hard, so the increase in accuracy will not be very high.\n",
    "* However, this is still quite significant considering the fact that the ConvNets we've used are relatively simple while the accuracy achieved using K-NN is with a search over 60,000 training images for every test image.\n",
    "* You can look at the performance of various machine learning methods on this problem at http://yann.lecun.com/exdb/mnist/\n",
    "* You can learn more about PyTorch at https://pytorch.org/tutorials/index.html\n",
    "* You can find another image classifier training example at https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
    "* You can play with a demo of neural network created by Daniel Smilkov and Shan Carter at https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
